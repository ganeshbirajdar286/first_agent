import { HumanMessage } from "@langchain/core/messages";
import { MessagesAnnotation, StateGraph } from "@langchain/langgraph";
import readline from "node:readline/promises"
import { ChatGroq } from "@langchain/groq";


/*
  1) define node function
  2) build the graph
  3)complie and invoke the graph
*/


const r1=readline.createInterface({
    input:process.stdin,// this take input for terminal
    output:process.stdout
});

/*
 initalise  the LLM
*/
const llm = new ChatGroq({
  apiKey: process.env.GROQ_API_KEY, 
  model: "openai/gpt-oss-120b",
  temperature:0,
  maxRetries:2

});


//  1) define node function
async function callModel(state){   //state has value of messages:[{role:"user",content:userInput}]
    // call the llm using API  

    console.log("calling LLM...");

    const response=await llm.invoke(state.messages);
    return {messages:[response ]} // this response is return will go in langGraph state 
    //  messages:[response ] this is only write in langraph and langchain it  handle internally merge 


//before return 
//     state.messages = [
//   { role: "user", content: "Hi" }
// ]

// after return 
//state.messages = [
//   { role: "user", content: "Hi" },
//   { role: "assistant", content: "Hello! How can I help you?" }
// ]


   
}

// 2) build the graph
const workflow =new StateGraph(MessagesAnnotation).addNode("agent",callModel).addEdge("__start__","agent").addEdge("agent","__end__");

//3)complie 
const app=workflow.compile()

async function main() {
    while(true){
        
        const userInput=await r1.question("You:");
        if(userInput==="/bye") break;
       //4)invoke the graph
      const finalstate= await app.invoke({
           messages:[{role:"user",content:userInput}]
       })
       const lastmessage=finalstate.messages[finalstate.messages.length - 1];
        console.log("AI:",lastmessage.content);
    }
    r1.close();
}

main()

/* 
================= HOW THIS CODE RUNS (STEP-WISE) =================

1Ô∏è‚É£ Readline starts
   ‚Üí Terminal chat interface is created to take user input.

2Ô∏è‚É£ LLM initialised
   ‚Üí ChatGroq model is configured with API key + model.

3Ô∏è‚É£ Graph built
   ‚Üí StateGraph created with:
        START ‚Üí agent ‚Üí END

4Ô∏è‚É£ Graph compiled
   ‚Üí Workflow converted into runnable app.

5Ô∏è‚É£ User enters input
   ‚Üí Example: "Hello"

6Ô∏è‚É£ Graph invoked
   ‚Üí User message passed into LangGraph state.

7Ô∏è‚É£ Agent node executes
   ‚Üí callModel() runs
   ‚Üí "calling LLM..." printed.

8Ô∏è‚É£ LLM API called
   ‚Üí llm.invoke(state.messages)

9Ô∏è‚É£ LLM response returned
   ‚Üí Assistant message generated.

üîü State updated
   ‚Üí LangGraph auto-merges:
        user + assistant messages.

1Ô∏è‚É£1Ô∏è‚É£ Final state received
   ‚Üí Returned from graph execution.

1Ô∏è‚É£2Ô∏è‚É£ Last message extracted
   ‚Üí Get assistant reply from messages array.

1Ô∏è‚É£3Ô∏è‚É£ AI response printed
   ‚Üí console.log("AI:", lastmessage.content)

1Ô∏è‚É£4Ô∏è‚É£ Loop repeats
   ‚Üí Until user types "/bye"

===============================================================
*/
