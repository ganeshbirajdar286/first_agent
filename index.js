import { HumanMessage } from "@langchain/core/messages";
import { MemorySaver, MessagesAnnotation, StateGraph } from "@langchain/langgraph";
import readline from "node:readline/promises"
import { ChatGroq } from "@langchain/groq";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import {TavilySearch} from "@langchain/tavily"
import { threadId } from "node:worker_threads";


// Memory checkpointer â†’ saves chat history per thread
const checkPointer=new MemorySaver()
/*
initailise the tool node
*/
const tool=new TavilySearch({
   maxResults:3,
   topic:"general"
});
const tools=[tool]
const toolNode =new ToolNode(tools)

/*
  1) define node function
  2) build the graph
  3)complie and invoke the graph
*/


const r1=readline.createInterface({
    input:process.stdin,// this take input for terminal
    output:process.stdout
});

/*
 initalise  the LLM
*/
const llm = new ChatGroq({
  apiKey: process.env.GROQ_API_KEY, 
  model: "openai/gpt-oss-120b",
  temperature:0,
  maxRetries:2

}).bindTools(tools)


//  1) define node function
async function callModel(state){   //state has value of messages:[{role:"user",content:userInput}]
    // call the llm using API  

    console.log("calling LLM...");

    const response=await llm.invoke(state.messages);
    return {messages:[response ]} // this response is return will go in langGraph state 
    //  messages:[response ] this is only write in langraph and langchain it  handle internally merge 


//before return 
//     state.messages = [
//   { role: "user", content: "Hi" }
// ]

// after return 
//state.messages = [
//   { role: "user", content: "Hi" },
//   { role: "assistant", content: "Hello! How can I help you?" }
// ]


   
}

function shouldContinue(state){
   // whether to call tool or end
   const lastmessage=state.messages[state.messages.length - 1];
   if(lastmessage.tool_calls.length > 0){
      return "tools"
   }
  return "__end__"
}

// 2) build the graph
const workflow =new StateGraph(MessagesAnnotation).addNode("agent",callModel).addNode("tools",toolNode).addEdge("__start__","agent").addEdge("tools","agent").addConditionalEdges("agent",shouldContinue);

//3)complie 
const app = workflow.compile({ checkpointer: checkPointer /* Enable memory saving*/   })


async function main() {
    while(true){
        
        const userInput=await r1.question("You:");
        if(userInput==="/bye") break;
       //4)invoke the graph
      const finalstate= await app.invoke({
           messages:[{role:"user",content:userInput}]
       },{configurable:{thread_id:"1"}})
       const lastmessage=finalstate.messages[finalstate.messages.length - 1];
        console.log("AI:",lastmessage.content);
    }
    r1.close();
}

main()

/* 
================= HOW THIS AGENT WORKS (FULL FLOW) =================

ğŸ§  OVERVIEW
This is a Tool-Calling AI Agent built using:

- LangGraph â†’ Workflow engine
- LangChain â†’ LLM + Tools wrapper
- Groq â†’ Model provider
- Tavily â†’ Search tool
- Readline â†’ Terminal chat UI

Flow:
User â†’ Agent(LLM) â†’ Tool (if needed) â†’ Agent â†’ Output


---------------------------------------------------------------
1ï¸âƒ£ TOOL INITIALISATION
---------------------------------------------------------------

const tool = new TavilySearch({...})

â†’ Tavily is a web search tool
â†’ maxResults: 3 â†’ returns top 3 search results
â†’ topic: "general" â†’ general web search

tools array created:

const tools = [tool]

Then wrapped inside ToolNode:

const toolNode = new ToolNode(tools)

ToolNode allows LangGraph to execute tools.


---------------------------------------------------------------
2ï¸âƒ£ LLM INITIALISATION
---------------------------------------------------------------

const llm = new ChatGroq({...}).bindTools(tools)

â†’ ChatGroq connects to Groq API
â†’ Model: openai/gpt-oss-120b
â†’ temperature: 0 â†’ deterministic output

.bindTools(tools) does:

LLM now knows:

â€¢ What tools exist  
â€¢ When to call them  
â€¢ How to format tool_calls


---------------------------------------------------------------
3ï¸âƒ£ NODE FUNCTION â€” AGENT
---------------------------------------------------------------

async function callModel(state)

state contains:

{
  messages: [
    { role: "user", content: "..." }
  ]
}

Steps:

1. Print â†’ "calling LLM..."
2. Send messages â†’ llm.invoke()
3. LLM generates response
4. Return:

return { messages: [response] }

LangGraph auto-merges messages.


---------------------------------------------------------------
4ï¸âƒ£ CONDITIONAL EDGE LOGIC
---------------------------------------------------------------

function shouldContinue(state)

Purpose:
Decide next step.

Steps:

1. Get last message:

const lastmessage = state.messages.at(-1)

2. Check tool calls:

if (lastmessage.tool_calls.length > 0)

If TRUE â†’
   go to "tools" node

If FALSE â†’
   end graph (__end__)


---------------------------------------------------------------
5ï¸âƒ£ GRAPH BUILDING
---------------------------------------------------------------

const workflow = new StateGraph(MessagesAnnotation)

Nodes added:

.addNode("agent", callModel)
.addNode("tools", toolNode)

Edges:

START â†’ agent
tools â†’ agent   (loop back after tool runs)

Conditional:

agent â†’ tools OR end


Graph structure:

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
START â†’ â”‚  Agent   â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
             â”‚
     tool_calls ?
        YES â”‚ NO
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
      â”‚   Tools    â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
             â”‚
             â””â”€â”€â”€â”€â†’ Agent â†’ END


---------------------------------------------------------------
6ï¸âƒ£ GRAPH COMPILE
---------------------------------------------------------------

const app = workflow.compile()

â†’ Converts graph â†’ runnable app


---------------------------------------------------------------
7ï¸âƒ£ RUNTIME LOOP
---------------------------------------------------------------

while(true)

Terminal chat runs continuously.

User input taken:

const userInput = await r1.question("You:")


---------------------------------------------------------------
8ï¸âƒ£ GRAPH INVOCATION
---------------------------------------------------------------

const finalstate = await app.invoke({
  messages: [
    { role: "user", content: userInput }
  ]
})

Graph execution starts.


---------------------------------------------------------------
9ï¸âƒ£ EXECUTION SCENARIOS
---------------------------------------------------------------

CASE 1 â€” No Tool Needed

User: "Hello"

Flow:

Agent â†’ LLM â†’ Response â†’ END


CASE 2 â€” Tool Needed

User: "Search latest AI news"

Flow:

Agent â†’ LLM decides tool_call â†’
Tools Node executes Tavily â†’
Results returned â†’
Agent summarises â†’
END


---------------------------------------------------------------
ğŸ”Ÿ STATE AUTO-MERGE
---------------------------------------------------------------

Before:

[
  { role: "user", content: "Hi" }
]

After Agent:

[
  { role: "user", content: "Hi" },
  { role: "assistant", content: "Hello!" }
]

After Tool:

[
  user,
  assistant(tool_call),
  tool_result,
  assistant(final_answer)
]


---------------------------------------------------------------
1ï¸âƒ£1ï¸âƒ£ FINAL OUTPUT
---------------------------------------------------------------

const lastmessage =
  finalstate.messages[finalstate.messages.length - 1];

console.log("AI:", lastmessage.content);

Prints final AI reply.


---------------------------------------------------------------
1ï¸âƒ£2ï¸âƒ£ LOOP EXIT
---------------------------------------------------------------

If user types:

/bye

Loop breaks â†’ readline closes.


===============================================================
ğŸ”š SUMMARY
===============================================================

â€¢ Readline â†’ takes user input
â€¢ Graph â†’ controls workflow
â€¢ Agent â†’ calls LLM
â€¢ LLM â†’ may call tools
â€¢ ToolNode â†’ executes tools
â€¢ State â†’ stores conversation
â€¢ Conditional edges â†’ decide flow

This creates a fully functional Tool-Calling AI Agent.
===============================================================
*/
